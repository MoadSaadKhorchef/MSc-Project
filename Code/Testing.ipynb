{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/moad.s.k/hello-world/e/HEL-3\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for the remaining 54 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 54 operations synced, thanks for waiting!\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune.\n",
      "Experiencing connection interruptions. Will try to reestablish communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n",
      "Experiencing connection interruptions. Reestablishing communication with Neptune.\n"
     ]
    }
   ],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "run = neptune.init(project='moad.s.k/hello-world', \n",
    "                   api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1MWE3YmU1ZC1jNWViLTQ1ZjUtYTE4NC1mOTgzYmUxNmI5MDkifQ==') # your credentials\n",
    "\n",
    "# Track metadata and hyperparameters of your Run\n",
    "run[\"JIRA\"] = \"NPT-952\"\n",
    "run[\"algorithm\"] = \"ConvNet\"\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\"\n",
    "}\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "\n",
    "# Track the training process by logging your training metrics\n",
    "for epoch in range(100):\n",
    "    run[\"train/accuracy\"].log(epoch * 0.6)\n",
    "    run[\"train/loss\"].log(epoch * 0.4)\n",
    "\n",
    "# Log the final results\n",
    "run[\"f1_score\"] = 0.66\n",
    "\n",
    "# Stop logging to your Run\n",
    "import os\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = os.path.normpath(\"sentinel-river-segmentation-dataset/\")\n",
    "x_train_dir = os.path.join(dataset_dir,\"x_train\")\n",
    "y_train_dir = os.path.join(dataset_dir,\"y_train\")\n",
    "x_test_dir = os.path.join(dataset_dir,\"x_test\")\n",
    "y_test_dir = os.path.join(dataset_dir,\"y_test\")\n",
    "\n",
    "train_set = Dataset(x_train_dir, y_train_dir, input_size=PARAMS['input_size'], output_size=PARAMS['output_size'], n_classes=PARAMS[\"n_classes\"], count=PARAMS[\"train_dataset_size\"])\n",
    "test_set = Dataset(x_test_dir, y_test_dir, input_size=PARAMS['input_size'], output_size=PARAMS['output_size'], n_classes=PARAMS[\"n_classes\"], count=PARAMS[\"test_dataset_size\"])\n",
    "\n",
    "batch_size = PARAMS['batch_size']\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=PARAMS['batch_size'], shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(test_set, batch_size=PARAMS['batch_size'], shuffle=True, num_workers=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentinel-river-segmentation-dataset\\\\x_train'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "#dataset configuration\n",
    "dataset_dir = os.path.normpath(\"sentinel-river-segmentation-dataset\")\n",
    "x_train_dir = os.path.join(dataset_dir,\"x_train\")\n",
    "y_train_dir = os.path.join(dataset_dir,\"y_train\")\n",
    "x_test_dir = os.path.join(dataset_dir,\"x_test\")\n",
    "y_test_dir = os.path.join(dataset_dir,\"y_test\")\n",
    "\n",
    "x_train_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Moad\\\\Documents\\\\MSc Project\\\\Code'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"sentinel-river-segmentation-dataset\\\\x_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
